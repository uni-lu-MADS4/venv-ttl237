from nltk.tokenize import word_tokenize
text = "This is a simple sentence."
tokens = word_tokenize(text)
print(f"{len(tokens)} tokens found, the tokens are: {tokens}")
